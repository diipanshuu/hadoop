version: '3.8'

services:
  namenode:
    image: apache/hadoop:3.3.6
    container_name: hadoop-namenode
    hostname: namenode
    ports:
      - "9870:9870"  # NameNode Web UI
      - "8020:8020"  # NameNode RPC
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_nameservices=hdfs-cluster
      - HDFS_CONF_dfs_ha_namenodes_hdfs___cluster=namenode
      - HDFS_CONF_dfs_namenode_rpc___address_hdfs___cluster_namenode=namenode:8020
      - HDFS_CONF_dfs_namenode_http___address_hdfs___cluster_namenode=namenode:9870
      - HDFS_CONF_dfs_replication=2
      - HDFS_CONF_dfs_namenode_name_dir=/hadoop/dfs/name
    volumes:
      - namenode-data:/hadoop/dfs/name
      - ./hadoop-config:/opt/hadoop/etc/hadoop
    networks:
      - hadoop-network
    command: ["hdfs", "namenode"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9870/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  datanode1:
    image: apache/hadoop:3.3.6
    container_name: hadoop-datanode1
    hostname: datanode1
    ports:
      - "9864:9864"  # DataNode Web UI
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_datanode_data_dir=/hadoop/dfs/data
      - HDFS_CONF_dfs_datanode_http___address=0.0.0.0:9864
      - HDFS_CONF_dfs_replication=2
    volumes:
      - datanode1-data:/hadoop/dfs/data
      - ./hadoop-config:/opt/hadoop/etc/hadoop
    networks:
      - hadoop-network
    depends_on:
      namenode:
        condition: service_healthy
    command: ["hdfs", "datanode"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9864/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  datanode2:
    image: apache/hadoop:3.3.6
    container_name: hadoop-datanode2
    hostname: datanode2
    ports:
      - "9865:9864"  # DataNode Web UI (different host port)
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_datanode_data_dir=/hadoop/dfs/data
      - HDFS_CONF_dfs_datanode_http___address=0.0.0.0:9864
      - HDFS_CONF_dfs_replication=2
    volumes:
      - datanode2-data:/hadoop/dfs/data
      - ./hadoop-config:/opt/hadoop/etc/hadoop
    networks:
      - hadoop-network
    depends_on:
      namenode:
        condition: service_healthy
    command: ["hdfs", "datanode"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9864/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  resourcemanager:
    image: apache/hadoop:3.3.6
    container_name: hadoop-resourcemanager
    hostname: resourcemanager
    ports:
      - "8088:8088"  # ResourceManager Web UI
      - "8032:8032"  # ResourceManager RPC
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - YARN_CONF_yarn_resourcemanager_bind___host=0.0.0.0
      - YARN_CONF_yarn_resourcemanager_address=resourcemanager:8032
      - YARN_CONF_yarn_resourcemanager_scheduler___address=resourcemanager:8030
      - YARN_CONF_yarn_resourcemanager_resource___tracker___address=resourcemanager:8031
      - YARN_CONF_yarn_resourcemanager_admin___address=resourcemanager:8033
      - YARN_CONF_yarn_resourcemanager_webapp___address=resourcemanager:8088
      - YARN_CONF_yarn_timeline___service_enabled=false
      - YARN_CONF_mapreduce_map_output_compress=true
      - YARN_CONF_mapred_map_output_compress_codec=org.apache.hadoop.io.compress.SnappyCodec
      - YARN_CONF_yarn_nodemanager_resource_memory___mb=4096
      - YARN_CONF_yarn_scheduler_maximum___allocation___mb=4096
      - YARN_CONF_yarn_nodemanager_vmem___check___enabled=false
    volumes:
      - ./hadoop-config:/opt/hadoop/etc/hadoop
    networks:
      - hadoop-network
    depends_on:
      namenode:
        condition: service_healthy
    command: ["yarn", "resourcemanager"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8088/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  nodemanager1:
    image: apache/hadoop:3.3.6
    container_name: hadoop-nodemanager1
    hostname: nodemanager1
    ports:
      - "8042:8042"  # NodeManager Web UI
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - YARN_CONF_yarn_resourcemanager_address=resourcemanager:8032
      - YARN_CONF_yarn_resourcemanager_scheduler___address=resourcemanager:8030
      - YARN_CONF_yarn_resourcemanager_resource___tracker___address=resourcemanager:8031
      - YARN_CONF_yarn_nodemanager_aux___services=mapreduce_shuffle
      - YARN_CONF_yarn_nodemanager_aux___services_mapreduce___shuffle_class=org.apache.hadoop.mapred.ShuffleHandler
      - YARN_CONF_yarn_nodemanager_bind___host=0.0.0.0
      - YARN_CONF_yarn_nodemanager_hostname=nodemanager1
      - YARN_CONF_yarn_nodemanager_webapp___address=nodemanager1:8042
      - YARN_CONF_yarn_nodemanager_resource_memory___mb=4096
      - YARN_CONF_yarn_nodemanager_resource_cpu___vcores=4
      - YARN_CONF_yarn_nodemanager_vmem___check___enabled=false
      - YARN_CONF_yarn_nodemanager_disk___health___checker_enable=false
    volumes:
      - ./hadoop-config:/opt/hadoop/etc/hadoop
    networks:
      - hadoop-network
    depends_on:
      resourcemanager:
        condition: service_healthy
    command: ["yarn", "nodemanager"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8042/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  historyserver:
    image: apache/hadoop:3.3.6
    container_name: hadoop-historyserver
    hostname: historyserver
    ports:
      - "19888:19888"  # History Server Web UI
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - MAPRED_CONF_mapreduce_framework_name=yarn
      - MAPRED_CONF_mapreduce_jobhistory_address=historyserver:10020
      - MAPRED_CONF_mapreduce_jobhistory_webapp_address=historyserver:19888
      - MAPRED_CONF_mapreduce_jobhistory_intermediate___done___dir=/mr-history/tmp
      - MAPRED_CONF_mapreduce_jobhistory_done___dir=/mr-history/done
    volumes:
      - ./hadoop-config:/opt/hadoop/etc/hadoop
    networks:
      - hadoop-network
    depends_on:
      namenode:
        condition: service_healthy
    command: ["mapred", "historyserver"]

  hue:
    image: gethue/hue:latest
    container_name: hadoop-hue
    hostname: hue
    ports:
      - "8888:8888"  # Hue Web UI
    environment:
      - HUE_CONF_desktop_secret_key=hue_secret_key_change_me
      - HUE_CONF_desktop_http_host=0.0.0.0
      - HUE_CONF_desktop_http_port=8888
      - HUE_CONF_desktop_time_zone=UTC
      - HUE_CONF_hadoop_hdfs_clusters_default_fs_defaultfs=hdfs://namenode:8020
      - HUE_CONF_hadoop_hdfs_clusters_default_webhdfs_url=http://namenode:9870/webhdfs/v1
      - HUE_CONF_hadoop_yarn_clusters_default_resourcemanager_api_url=http://resourcemanager:8088
      - HUE_CONF_hadoop_yarn_clusters_default_history_server_api_url=http://historyserver:19888
    volumes:
      - ./hue-config:/usr/share/hue/desktop/conf
      - hue-data:/usr/share/hue/data
    networks:
      - hadoop-network
    depends_on:
      - namenode
      - resourcemanager
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8888/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  namenode-data:
    name: hadoop-namenode-data
  datanode1-data:
    name: hadoop-datanode1-data
  datanode2-data:
    name: hadoop-datanode2-data
  hue-data:
    name: hadoop-hue-data

networks:
  hadoop-network:
    name: hadoop-network
    driver: bridge